{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Import Statements\n",
    "# .............................\n",
    "#%matplotlib inline\n",
    "import matplotlib; matplotlib.use('agg')\n",
    "\n",
    "import netCDF4\n",
    "import shapely\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy.odr as odr\n",
    "import scipy.signal as signal\n",
    "import scipy.integrate as integrate\n",
    "import importlib\n",
    "import datetime as dt\n",
    "import mpl_toolkits.basemap as Basemap\n",
    "\n",
    "# Imports for Polygon Routines\n",
    "# ..................................\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "from shapely import geometry\n",
    "from shapely import ops\n",
    "from decimal import Decimal\n",
    "# .............................\n",
    "\n",
    "def ncdump(nc_fid, verb=True):\n",
    "    '''\n",
    "    ncdump outputs dimensions, variables and their attribute information.\n",
    "    The information is similar to that of NCAR's ncdump utility.\n",
    "    ncdump requires a valid instance of Dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nc_fid : netCDF4.Dataset\n",
    "        A netCDF4 dateset object\n",
    "    verb : Boolean\n",
    "        whether or not nc_attrs, nc_dims, and nc_vars are printed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nc_attrs : list\n",
    "        A Python list of the NetCDF file global attributes\n",
    "    nc_dims : list\n",
    "        A Python list of the NetCDF file dimensions\n",
    "    nc_vars : list\n",
    "        A Python list of the NetCDF file variables\n",
    "    '''\n",
    "    def print_ncattr(key):\n",
    "        \"\"\"\n",
    "        Prints the NetCDF file attributes for a given key\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key : unicode\n",
    "            a valid netCDF4.Dataset.variables key\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print (\"\\t\\ttype:\", repr(nc_fid.variables[key].dtype))\n",
    "            for ncattr in nc_fid.variables[key].ncattrs():\n",
    "                print ('\\t\\t%s:' % ncattr,\\\n",
    "                      repr(nc_fid.variables[key].getncattr(ncattr)))\n",
    "        except KeyError:\n",
    "            print (\"\\t\\tWARNING: %s does not contain variable attributes\") % key\n",
    "\n",
    "    # NetCDF global attributes\n",
    "    nc_attrs = nc_fid.ncattrs()\n",
    "    if verb:\n",
    "        print (\"NetCDF Global Attributes:\")\n",
    "        for nc_attr in nc_attrs:\n",
    "            print ('\\t%s:' % nc_attr, repr(nc_fid.getncattr(nc_attr)))\n",
    "    nc_dims = [dim for dim in nc_fid.dimensions]  # list of nc dimensions\n",
    "    # Dimension shape information.\n",
    "    if verb:\n",
    "        print (\"NetCDF dimension information:\")\n",
    "        for dim in nc_dims:\n",
    "            print (\"\\tName:\", dim) \n",
    "            print (\"\\t\\tsize:\", len(nc_fid.dimensions[dim]))\n",
    "            print_ncattr(dim)\n",
    "    # Variable information.\n",
    "    nc_vars = [var for var in nc_fid.variables]  # list of nc variables\n",
    "    if verb:\n",
    "        print (\"NetCDF variable information:\")\n",
    "        for var in nc_vars:\n",
    "            if var not in nc_dims:\n",
    "                print ('\\tName:', var)\n",
    "                print (\"\\t\\tdimensions:\", nc_fid.variables[var].dimensions)\n",
    "                print (\"\\t\\tsize:\", nc_fid.variables[var].size)\n",
    "                print_ncattr(var)\n",
    "    return nc_attrs, nc_dims, nc_vars\n",
    "\n",
    "# Defined Functions to be used in script below\n",
    "# .................................................\n",
    "\n",
    "# Function for grabbing coordinates 2PVU contour path\n",
    "def get_path_coord(contour):\n",
    "    paths = []\n",
    "    paths = contour.collections[0].get_paths()\n",
    "    n = len(paths)\n",
    "    xy = []\n",
    "    length = []\n",
    "    for i in np.arange(0,n,1):\n",
    "        length.append(len(paths[i]))\n",
    "    r = np.argmax(length) # gives index of largest contour path\n",
    "    \n",
    "    paths1 = paths[r]\n",
    "    xy = paths1.vertices\n",
    "\n",
    "    for i in range(len(xy)):\n",
    "        a = xy[:,0]\n",
    "        b = xy[:,1]\n",
    "    return a, b\n",
    "# ..................................\n",
    "# Function for finding the index of the closes value\n",
    "# Input Parameter\n",
    "# ..................\n",
    "# A: Array we want the index from\n",
    "#target: The array we would like to know the indices for\n",
    "\n",
    "def getnearpos(array,value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx   \n",
    "\n",
    "# ..................................\n",
    "# Python code to find longest running \n",
    "# sequence of positive integers.\n",
    "\n",
    "# Problem with code. Doesn't seem to see last large positive consec\n",
    "# group. Pad data at the end with negative values.\n",
    " \n",
    "def getLongestSeq(a, n):\n",
    "    maxIdx = 0\n",
    "    maxLen = 0\n",
    "    currLen = 0\n",
    "    currIdx = 0\n",
    "    \n",
    "    #check to see if data needs padding at the end\n",
    "    for k in range(n):\n",
    "        if a[k] > 0:\n",
    "            currLen +=1\n",
    " \n",
    "            # New sequence, store\n",
    "            # beginning index.\n",
    "            if currLen == 1:\n",
    "                currIdx = k\n",
    "        else:\n",
    "            if currLen > maxLen:\n",
    "                maxLen = currLen\n",
    "                maxIdx = currIdx\n",
    "            currLen = 0\n",
    "             \n",
    "    if maxLen > 0:\n",
    "        print('Index : ',maxIdx,',Length : ',maxLen,)\n",
    "    else:\n",
    "        print(\"No positive sequence detected.\")\n",
    "    return maxIdx, maxLen\n",
    "\n",
    "# ...............................\n",
    "# Function for collecting start and end points of contour\n",
    "# Input Parameters\n",
    "# ...........................\n",
    "# strt_idx: index of first point of pv gradient reversal\n",
    "# maxl: length of pv gradient reversal\n",
    "# last: index of last value of array\n",
    "\n",
    "def start_end_contour(fidx,maxl,last):    \n",
    "    md_idx = fidx\n",
    "    end_idx = md_idx + maxl\n",
    "    strt_idx = md_idx - maxl\n",
    "    \n",
    "    if end_idx > last:\n",
    "        if strt_idx < 0:\n",
    "            strt_idx1 = 0\n",
    "            return strt_idx1, md_idx,last\n",
    "        else:\n",
    "            return strt_idx, md_idx, last\n",
    "    else:\n",
    "        if strt_idx < 0:\n",
    "            strt_idx1 = 0\n",
    "            return strt_idx1, md_idx, end_idx\n",
    "        else:\n",
    "            return strt_idx, md_idx, end_idx\n",
    "\n",
    "# ......................................        \n",
    "# Function for retrieving path of contour and create Polygon\n",
    "# Algorithm for retrieving 2-PVU contour paths for each \n",
    "# Retrieve 2-PVU contour path.\n",
    "\n",
    "def retrieve_path(contour):\n",
    "    paths = []\n",
    "    paths = contour.collections[0].get_paths()\n",
    "    n = len(paths)\n",
    "    xy = []\n",
    "    length = []\n",
    "    for i in np.arange(0,n,1):\n",
    "        length.append(len(paths[i]))\n",
    "    r = np.argmax(length) # gives index of largest contour path\n",
    "    \n",
    "    paths1 = paths[r]\n",
    "    xy = paths1.vertices\n",
    "\n",
    "    for i in range(len(xy)):\n",
    "        a = xy[:,0]\n",
    "        b = xy[:,1]\n",
    "    poly = geometry.Polygon([(i[0],i[1]) for i in zip(a,b)])\n",
    "    \n",
    "    # find intersection between line and contour segment\n",
    "    \n",
    "    return poly\n",
    "\n",
    "# .............................\n",
    "# Function for choosing the start point of the PV streamer segment\n",
    "# Code tests whether p2gradx2 is positive at positive p2grady2\n",
    "# Input Variables:\n",
    "# firstx, firsty: index of first value indicated by getLongSeq\n",
    "# lengthx, lengthy: length of consecutive group of positive values indicated by getLongSeq\n",
    "\n",
    "# Output Variables:\n",
    "# strt_idx, end-idx: start and end index bounds of PV streamer\n",
    "# .........................................\n",
    "\n",
    "def pvindex_bnds(datax,datay,firstx,firsty,lengthx,lengthy):\n",
    "    idx1 = np.min([firstx,firsty])  # Idnetify range over which to search\n",
    "    idx2 = np.max([firstx,firsty])+1\n",
    "    irange = np.arange(idx1,idx2,1)\n",
    "\n",
    "    for i in irange:     # Search along positive p2grady2 for index with positive p2gradx2\n",
    "        if (datay[i]>0 and datax[i]>0):\n",
    "            strt_idx = i\n",
    "            pos_length = np.max([lengthx,lengthy])  # Find index detection length (pos_length x 2) of the PV streamer.\n",
    "            end_idx = strt_idx + (pos_length*2)     # Find end index. \n",
    "            mdl_idx = strt_idx + np.min([lengthx,lengthy]) # Find middle index to be used to define PV streamer region.\n",
    "            \n",
    "            if end_idx >= len(p2grady2):            # Check whether end_idx is greater than length of contour path\n",
    "                end_idx = len(p2grady2)-2\n",
    "            else:\n",
    "                end_idx = end_idx\n",
    "            \n",
    "            if mdl_idx >= len(p2grady2):            # Check whether mdl_idx is greater than length of contour path\n",
    "                mdl_idx = len(p2grady2)-2\n",
    "            else:\n",
    "                mdl_idx = mdl_idx\n",
    "            break\n",
    "        else:\n",
    "            strt_idx = float('NaN')\n",
    "            mdl_idx = float('NaN')\n",
    "            end_idx = float('NaN')\n",
    "            pass\n",
    "            \n",
    "    return strt_idx, mdl_idx, end_idx\n",
    "\n",
    "# ....................................\n",
    "# Function\n",
    "# Pad data with negatives at the end. Section tests whether or not the very last element is positive or negative,\n",
    "# since getLongSec function doesn't \"see\" positive end values very well unless part of a significantly large\n",
    "# sequence. If end value is positive (i.e. p2grady2[-1]>0), then test is padded with -1's.\n",
    "# ...........................................................\n",
    "# Store size of p2gradx2 and p2grady2\n",
    "\n",
    "def data_pad(datax,datay):\n",
    "    # Tests if padding needed\n",
    "    if (datay[-1]>0 or datax[-1]>0):\n",
    "        testy = np.copy(datay)\n",
    "        testx = np.copy(datax)\n",
    "        pad = np.repeat(-1,5)\n",
    "        testy2 = np.hstack([testy,pad])\n",
    "        testx2 = np.hstack([testx,pad])\n",
    "    else:\n",
    "        testy2 = np.copy(datay)\n",
    "        testx2 = np.copy(datax)\n",
    "    \n",
    "    return testx2, testy2\n",
    "\n",
    "# Function for calculating the fourier coefficents (Ao, Ak, and Bk),\n",
    "# variance explained (Ck squared) and the sample frequencies. \n",
    "# Currently one-dimensional.\n",
    "def fft_coeff_numpy(f,T):\n",
    "\n",
    "# Input Parameters\n",
    "# ....................\n",
    "# f: n-dimensional data\n",
    "# T: length of window/data\n",
    "\n",
    "# Output Parameters\n",
    "# ....................\n",
    "# \n",
    "\n",
    "# Record length must be odd, so here we test whether the data length\n",
    "# is even or odd. If it is odd, T remains. If it is even, T becomes T-1.\n",
    "    if T % 2 == 0:\n",
    "        T = T-1 # even, subtract 1\n",
    "    else:\n",
    "        T = T # odd, nothing changes\n",
    "    \n",
    "\n",
    "# Size of data. T must be odd. Therefore, t is also odd. But, N is even.\n",
    "    dt = 1 # change to aquire different sample rate\n",
    "    t= np.arange(0,T,dt)\n",
    "    N = np.size(t)-1\n",
    "\n",
    "\n",
    "# Define window size and frequencies\n",
    "    freq = fft.fftfreq(T,dt)\n",
    "    nyquist = N/(2*T)\n",
    "    \n",
    "# Calculate wavenumbers\n",
    "    T2 = N/2\n",
    "    k = np.arange(1,T2,1)\n",
    "    \n",
    "# Calculate fourier series\n",
    "    y1 = (fft.fft(f[np.arange(0,t.size,1)]))/t.size\n",
    "    y2 = fft.fft(f[np.arange(0,t.size,1)])\n",
    "\n",
    "# Calculate fourier coefficients\n",
    "    Ao = y1[0].real\n",
    "    Ak = y1[1:-1].real\n",
    "    Bk = -y1[1:-1].imag\n",
    "    C = np.absolute(y1)\n",
    "\n",
    "# Calculate fraction of variance explained and normalize\n",
    "    pve = ((C**2)/np.sum(C**2))    # scale with 2 to obtain area under curve of 1.\n",
    "\n",
    "    return y1, y2, freq, T2, nyquist, k, Ao, Ak, Bk, C, pve, T\n",
    "\n",
    "# ..................................\n",
    "# Function for bandpass filtering\n",
    "# Source: Libby\n",
    "# Currently set to use mean + first three harmonics\n",
    "# Input Variables\n",
    "# data: your time series or data\n",
    "# fidx: index/indices of desired harmonics/frequencies\n",
    "# ..................\n",
    "# Output Variables\n",
    "# lowpass-flt: time series of low-frequency portion of data\n",
    "# highpass_flt: time series of high-frequency portion of data\n",
    "\n",
    "# INCOMPLETE\n",
    "def fft_filter1(transform):\n",
    "    # Lowpass filter\n",
    "    dtlow = np.copy(transform)\n",
    "    dtlow[4:] = 0.\n",
    "    #dtlow[0:3] = 0.\n",
    "    \n",
    "    lowpass_flt = np.real(fft.ifft(dtlow))\n",
    "    \n",
    "    # Highpass filter\n",
    "    dthigh = np.copy(transform)\n",
    "    dthigh[0:3:] = 0.\n",
    "    #dthigh[-2::] = 0.\n",
    "    \n",
    "    highpass_flt = np.real(np.fft.ifft(dthigh))\n",
    "    \n",
    "    return lowpass_flt, highpass_flt\n",
    "\n",
    "# NC file input\n",
    "\n",
    "ncfile='/home/jjpjones/pvs_vws_indices/era5_pv350k_6hrly_v2.nc'\n",
    "ncf = Dataset(ncfile,'r')\n",
    "nc_attrs, nc_dims, nc_vars = ncdump(ncf)\n",
    "\n",
    "# Extract data from NetCDF file\n",
    "lat = ncf.variables['latitude'][:]\n",
    "lon = ncf.variables['longitude'][:]\n",
    "time = ncf.variables['time'][:]\n",
    "pv = ncf.variables['pv'][:]\n",
    "\n",
    "# List all times in file as datetime objects\n",
    "#time2 = [int(i) for i in time]\n",
    "#dt_time = [dt.date(1900,1,1) + dt.timedelta(hours=t) for t in time2]\n",
    "\n",
    "clim0, clim6, clim12, clim18 = np.empty([73,144]),np.empty([73,144]),np.empty([73,144]),np.empty([73,144])\n",
    "std0, std6, std12, std18 = np.empty([73,144]),np.empty([73,144]),np.empty([73,144]),np.empty([73,144])\n",
    "\n",
    "for j in np.arange(0,144,1):\n",
    "    for i in np.arange(0,73,1):\n",
    "        clim_range = pv[np.arange(0,58800,1),i,j]\n",
    "        dates = pd.date_range('1979-01-01','2019-04-01',freq='6H')\n",
    "        clim_frame = pd.Series(clim_range,index=dates[0:58800])\n",
    "        clim = clim_frame.groupby(clim_frame.index.hour).mean()\n",
    "        clim2 = np.array(clim.values)\n",
    "        pvstd = clim_frame.groupby(clim_frame.index.hour).std()\n",
    "        pvstd2 = np.array(pvstd.values)\n",
    "        \n",
    "        clim0[i,j], clim6[i,j], clim12[i,j], clim18[i,j] = clim2[0],clim2[1],clim2[2],clim2[3]\n",
    "        std0[i,j], std6[i,j], std12[i,j], std18[i,j] = pvstd2[0],pvstd2[1],pvstd2[2],pvstd2[3]\n",
    "        \n",
    "# Set up empty arrays\n",
    "pvsi_full = []\n",
    "strt1, mid1, end1 = [],[],[]\n",
    "strt2, mid2, end2 = [],[],[]\n",
    "\n",
    "for q in np.arange(0,58800,1):\n",
    "    # Define field and calculate meridional pv gradient field\n",
    "    pv_field = pv[q,0:73,0:143]\n",
    "    pv_diffx = np.gradient(pv_field, axis=0)\n",
    "    pv_diffy = np.gradient(pv_field, axis =1)\n",
    "    \n",
    "    # Plot 2PVU contour and retrieve coordinates along path\n",
    "    cs1 = []\n",
    "    cs1 = plt.contour(lon[104:143],lat[12:34],pv_field[12:34,104:143],levels=[2e-6])\n",
    "    a, b = get_path_coord(cs1)\n",
    "    # ...........................................\n",
    "    \n",
    "    # Retrieve indices for contour coordinates\n",
    "    \n",
    "    idxlt, idxln = [], []\n",
    "    for r in b:\n",
    "        idxlt.append(getnearpos(lat,r))\n",
    "    \n",
    "    for r in a:\n",
    "        idxln.append(getnearpos(lon,r))\n",
    "    \n",
    "    idxlt = np.flipud(np.array(idxlt)) # convert to array. Update v2: position indices flipped to move from \n",
    "    idxln = np.flipud(np.array(idxln)) # left to right  \n",
    "    # ...........................................\n",
    "    \n",
    "    # Retrieve pv gradient values corresponding to 2PVU contour\n",
    "    # Added array for zonal pv gradient field\n",
    "\n",
    "    p2gradx, p2grady = [],[]\n",
    "    p2gradx.append(pv_diffx[idxlt,idxln])\n",
    "    p2grady.append(pv_diffy[idxlt,idxln])\n",
    "    \n",
    "    # Convert lists to arrays\n",
    "    p2gradx = np.array(p2gradx)\n",
    "    p2grady = np.array(p2grady)\n",
    "    \n",
    "    # Change from hstack to vstack\n",
    "    p2gradx2 = p2gradx[0,:]\n",
    "    p2grady2 = p2grady[0,:]\n",
    "    \n",
    "    # ...........................................\n",
    "    # Update v2: Data padding moved to user-defined function data_pad. pvindex_bnds replaces\n",
    "    # start_end_contour.\n",
    "    # Find largest group of consecutive positive values\n",
    "    # Create temporary array (test) and pad data with negatives \n",
    "    # at the end.\n",
    "    \n",
    "    # Section tests whether or not the very last element is positive or negative\n",
    "    # since getLongSec function doesn't \"see\" positive end values very well. If \n",
    "    # end value is positive (i.e. p2grad2[-1]>0), then test is padded with -1's.\n",
    "    testx, testy = data_pad(p2gradx2,p2grady2) \n",
    "    \n",
    "    \n",
    "    # Check for a large group of consecutive positive integers in zonal meridional pv gradient\n",
    "    #test\n",
    "    pidxy1, plenty1 = getLongestSeq(testy,len(testy))\n",
    "    pidxx1, plentx1 = getLongestSeq(testx,len(testx))\n",
    "   \n",
    "    if (plenty1 !=0 and plentx1 !=0):\n",
    "        strt1, mid1, end1 = pvindex_bnds(testx,testy,pidxx1,pidxy1,plentx1,plenty1)\n",
    "    else:\n",
    "        print('Step pvindex_bnds: No primary PV detected at timestep: '+ str(q))\n",
    "        strt1 = float('NaN')\n",
    "        mid1 = float('NaN')\n",
    "        end1 = float('NaN')\n",
    "        pass\n",
    "\n",
    "    # ...........................................\n",
    "    # Check for second major pv reversal\n",
    "    # Check for a second major group of consecutive positive integers\n",
    "    # Update v2\n",
    "    \n",
    "    if np.isnan(strt1) == False:\n",
    "        \n",
    "        # Define start, middle, and end points for\n",
    "        # First PV\n",
    "        lts1, lns1 = idxlt[strt1], idxln[strt1]\n",
    "        ltm1, lnm1 = idxlt[mid1], idxln[mid1]\n",
    "        lte1, lne1 = idxlt[end1], idxln[end1]\n",
    "        \n",
    "        # Create box around detected PV. Find box edges of pv streamer region.\n",
    "        #flat1, flat2, llat1, llat2 = [], [], [], []\n",
    "        flat1, llat1 = np.min([lts1,ltm1,lte1])-4,np.max([lts1,ltm1,lte1])+4\n",
    "        flon1, llon1 = np.min([lns1,lnm1,lne1])-4,np.max([lns1,lnm1,lne1])+4\n",
    "        \n",
    "        dims1 = np.size(lon[flon1:llon1])*np.size(lat[flat1:llat1])\n",
    "        \n",
    "        # Check for a second major group of consecutive positive integers by\n",
    "        # removing first group (replaced with -1's)\n",
    "        zero_pad1 = np.abs(end1 - strt1)\n",
    "        y_remaining = np.copy(testy)\n",
    "        y_remaining[strt1:end1] = np.repeat(-1,zero_pad1)\n",
    "\n",
    "        x_remaining = np.copy(testx)\n",
    "        x_remaining[strt1:end1] = np.repeat(-1,zero_pad1)\n",
    "\n",
    "        pidxy2, plenty2 = getLongestSeq(y_remaining,len(y_remaining))\n",
    "        pidxx2, plentx2 = getLongestSeq(x_remaining,len(x_remaining))\n",
    "    \n",
    "        # Check to make sure that lengths of group are greater than 0. \n",
    "        if (plenty2 != 0 and plentx2 != 0):\n",
    "            strt2, mid2, end2 = pvindex_bnds(x_remaining,y_remaining,pidxx2,pidxy2,plentx2,plenty2)\n",
    "            \n",
    "            if np.isnan(strt2) == False:\n",
    "                # Position of start, middle, and end index of second PV\n",
    "                lts2, lns2 = idxlt[strt2], idxln[strt2]\n",
    "                ltm2, lnm2 = idxlt[mid2], idxln[mid2]\n",
    "                lte2, lne2 = idxlt[end2], idxln[end2]\n",
    "            \n",
    "                # Create box around second detected PV. \n",
    "                flat2, llat2 = np.min([lts2,ltm2,lte2])-4, np.max([lts2,ltm2,lte2])+4\n",
    "                flon2, llon2 = np.min([lns2,lnm2,lne2])-4, np.max([lns2,lnm2,lne2])+4\n",
    "            \n",
    "                # Calculate box dimensions.\n",
    "                dims2 = np.size(lon[flon2:llon2])*np.size(lat[flat2:llat2])\n",
    "            else:\n",
    "                flat2, llat2 = float('NaN'), float('NaN')\n",
    "                flon2, llon2 = float('NaN'), float('NaN')\n",
    "                dims2=0\n",
    "        else:\n",
    "            flat2, llat2 = float('NaN'), float('NaN')\n",
    "            flon2, llon2 = float('NaN'), float('NaN')\n",
    "            dims2=0\n",
    "    else:\n",
    "        flat1, llat1 = float('NaN'), float('NaN')\n",
    "        flon1, llon1 = float('NaN'), float('NaN')\n",
    "        \n",
    "        flat2, llat2 = float('NaN'), float('NaN')\n",
    "        flon2, llon2 = float('NaN'), float('NaN')\n",
    "        \n",
    "        dims1 = 0\n",
    "        dims2 = 0\n",
    "        pass\n",
    "    \n",
    "    # ................................\n",
    "    \n",
    "    if np.any(np.isnan([flat1,llat1,flon1,llon1])) == True:\n",
    "        pvpoly1 = geometry.Polygon()\n",
    "    else:\n",
    "        if not np.size(pv_field[flat1:llat1,flon1:llon1]) == dims1:\n",
    "            pvpoly1 = geometry.Polygon()\n",
    "        else:\n",
    "            plt.figure(1)\n",
    "            plt.clf()\n",
    "            cnt1 = plt.contour(lon[flon1:llon1],lat[flat1:llat1],pv_field[flat1:llat1,flon1:llon1],levels=[2e-6])\n",
    "            pvpoly1 = retrieve_path(cnt1)\n",
    "            \n",
    "    if np.any(np.isnan([flat2,llat2,flon2,llon2])) == True:\n",
    "        pvpoly2 = geometry.Polygon()\n",
    "    else:\n",
    "        if not np.size(pv_field[flat2:llat2,flon2:llon2]) == dims2:\n",
    "            pvpoly2 = geometry.Polygon() \n",
    "        else:\n",
    "            plt.figure(2)\n",
    "            plt.clf()\n",
    "            cnt2 = plt.contour(lon[flon2:llon2],lat[flat2:llat2],pv_field[flat2:llat2,flon2:llon2],levels=[2e-6])\n",
    "            plt.plot()\n",
    "            pvpoly2 = retrieve_path(cnt2)\n",
    "        \n",
    "    # .................................................\n",
    "    # Draw latitudes and longitudes for region\n",
    "    lt = lat[20:28]\n",
    "    ln = lon[104:143]\n",
    "\n",
    "    ln_st, ln_ed = [], []\n",
    "    lt_st, lt_ed = [], []\n",
    "\n",
    "    # latitude\n",
    "    for i in lt:\n",
    "        lidx1 = len(ln)-1\n",
    "        lt_st.append((ln[0],i))\n",
    "        lt_ed.append((ln[lidx1],i))\n",
    "\n",
    "    # longitudes\n",
    "    for i in ln:\n",
    "        lidx2 = len(lt)-1\n",
    "        ln_st.append((i,lt[0]))\n",
    "        ln_ed.append((i,lt[lidx2]))\n",
    "    # ..................................\n",
    "    # Create multiple Shapely lines and store into one string\n",
    "    # Create coords lists\n",
    "    lat_coords = [[x,y] for x,y in zip(lt_st,lt_ed)]\n",
    "    lon_coords = [[x,y] for x,y in zip(ln_st,ln_ed)]\n",
    "\n",
    "    # String for multiple lines\n",
    "    lt_lines = geometry.MultiLineString(lat_coords)\n",
    "    ln_lines = geometry.MultiLineString(lon_coords)\n",
    "    \n",
    "    # Intersection between latitudes and longitudes\n",
    "    lat_lon_intersec = lt_lines.intersection(ln_lines)\n",
    "    \n",
    "    # Find intersection between lat/lon intersections \n",
    "    # and pv polygon area\n",
    "    \n",
    "    if not (pvpoly1.is_empty) == True: \n",
    "        try:\n",
    "            FINAL_intersec1 = lat_lon_intersec.intersection(pvpoly1.buffer(1))\n",
    "        except Exception:\n",
    "            pass\n",
    "        pvs_coordinates1 = np.array(geometry.shape(geometry.mapping(FINAL_intersec1)))\n",
    "    else:\n",
    "        print('Step FINAL_intersec1: No polygon detected.')\n",
    "        pvs_coordinates1 = np.empty(shape=(0,0))\n",
    "   \n",
    "    if not (pvpoly2.is_empty) == True:\n",
    "        try:\n",
    "            FINAL_intersec2 = lat_lon_intersec.intersection(pvpoly2.buffer(1))\n",
    "        except Exception:\n",
    "            pass\n",
    "        pvs_coordinates2 = np.array(geometry.shape(geometry.mapping(FINAL_intersec2)))\n",
    "    else:\n",
    "        print('Step FINAL_intersec2: No 2nd polygon detected.')\n",
    "        pvs_coordinates2 = np.empty(shape=(0,0))\n",
    "        \n",
    "        \n",
    "    # Find coordinates\n",
    "    # Largest PV streamer\n",
    "    if pvs_coordinates1.size==0:\n",
    "        xp1, yp1 = [], []\n",
    "        pv_anom1 = float('NaN')\n",
    "        pass     \n",
    "    elif pvs_coordinates1.size > 2:\n",
    "        xp1 = pvs_coordinates1[:,0]\n",
    "        yp1 = pvs_coordinates1[:,1]\n",
    "    else:\n",
    "        xp1 = pvs_coordinates1[0]\n",
    "        yp1 = pvs_coordinates1[1]\n",
    "        \n",
    "    # Second Largest PV streamer\n",
    "    if pvs_coordinates2.size==0:\n",
    "        xp2, yp2 = [], []\n",
    "        pv_anom2 = float('NaN')\n",
    "        pass     \n",
    "    elif pvs_coordinates2.size > 2:\n",
    "        xp2 = pvs_coordinates2[:,0]\n",
    "        yp2 = pvs_coordinates2[:,1]\n",
    "    else:\n",
    "        xp2 = pvs_coordinates2[0]\n",
    "        yp2 = pvs_coordinates2[1]\n",
    "    \n",
    "    # Find indices for coordinates\n",
    "    x_idx1, y_idx1 = [], []\n",
    "    x_idx2, y_idx2 = [], []\n",
    "\n",
    "    # x values\n",
    "    if isinstance(xp1,list)==True:\n",
    "        if len(xp1) ==0:\n",
    "            pass\n",
    "        elif len(xp1)>1:\n",
    "            for i in xp1:\n",
    "                x_idx1.append(getnearpos(lon,i))\n",
    "        else:\n",
    "            x_idx1.append(getnearpos(lon,xp1))\n",
    "    else:\n",
    "        if xp1.size ==0:\n",
    "            pass\n",
    "        elif xp1.size>1:\n",
    "            for i in xp1:\n",
    "                x_idx1.append(getnearpos(lon,i))\n",
    "        else:\n",
    "            x_idx1.append(getnearpos(lon,xp1)) \n",
    "    \n",
    "    if isinstance(xp2,list) == True:\n",
    "        if len(xp2) ==0:\n",
    "            pass\n",
    "        elif len(xp2)>1:\n",
    "            for i in xp2:\n",
    "                x_idx2.append(getnearpos(lon,i))\n",
    "        else:\n",
    "            x_idx2.append(getnearpos(lon,xp2))\n",
    "    else:\n",
    "        if xp2.size ==0:\n",
    "            pass\n",
    "        elif xp2.size>1:\n",
    "            for i in xp2:\n",
    "                x_idx2.append(getnearpos(lon,i))\n",
    "        else:\n",
    "            x_idx2.append(getnearpos(lon,xp2)) \n",
    "        \n",
    "    # y values    \n",
    "    if isinstance(yp1,list) == True:\n",
    "        if len(yp1) ==0:\n",
    "            pass\n",
    "        elif len(yp1)>1:\n",
    "            for i in yp1:\n",
    "                y_idx1.append(getnearpos(lat,i))\n",
    "        else:\n",
    "            y_idx1.append(getnearpos(lat,yp1))\n",
    "    else:\n",
    "        if yp1.size ==0:\n",
    "            pass\n",
    "        elif yp1.size>1:\n",
    "            for i in yp1:\n",
    "                y_idx1.append(getnearpos(lat,i))\n",
    "        else:\n",
    "            y_idx1.append(getnearpos(lat,yp1)) \n",
    "    \n",
    "    if isinstance(yp2,list) == True:\n",
    "        if len(yp2) ==0:\n",
    "            pass\n",
    "        elif len(yp2)>1:\n",
    "            for i in yp2:\n",
    "                y_idx2.append(getnearpos(lat,i))\n",
    "        else:\n",
    "            y_idx2.append(getnearpos(lat,yp2))\n",
    "    else:\n",
    "        if yp2.size ==0:\n",
    "            pass\n",
    "        elif yp2.size>1:\n",
    "            for i in yp2:\n",
    "                y_idx2.append(getnearpos(lat,i))\n",
    "        else:\n",
    "            y_idx2.append(getnearpos(lat,xp2)) \n",
    "        \n",
    "    # .......................................\n",
    "    # Retrieve pv values (OBSOLETE)\n",
    "    #pvs_area_pv1, pvs_area_pv2 = [], []\n",
    "    #pvs_area_pv1 = [pv_field[y,x] for y,x in zip(y_idx1,x_idx1)]\n",
    "    #pvs_area_pv2 = [pv_field[y,x] for y,x in zip(y_idx2,x_idx2)]\n",
    "    \n",
    "    # Find standardized anomalies of grid points within polygon\n",
    "    if dates[q].hour == 0:\n",
    "        if (len(y_idx1)!=0 and len(x_idx1)!=0):\n",
    "            for y,x in zip(y_idx1, x_idx1):\n",
    "                pv_anomaly1 = ((pv_field[y,x]-clim0[y,x])/std0[y,x])\n",
    "        else:\n",
    "            pv_anomaly1 = np.repeat(float('NaN'),5)\n",
    "    elif dates[q].hour == 6:\n",
    "        if (len(y_idx1)!=0 and len(x_idx1)!=0):\n",
    "            for y,x in zip(y_idx1, x_idx1):\n",
    "                pv_anomaly1 = ((pv_field[y,x]-clim6[y,x])/std6[y,x])\n",
    "        else:\n",
    "            pv_anomaly1 = np.repeat(float('NaN'),5)\n",
    "    elif dates[q].hour == 12:\n",
    "        if (len(y_idx1)!=0 and len(x_idx1)!=0):\n",
    "            for y,x in zip(y_idx1, x_idx1):\n",
    "                pv_anomaly1 = ((pv_field[y,x]-clim12[y,x])/std12[y,x])\n",
    "        else:\n",
    "            pv_anomaly1 = np.repeat(float('NaN'),5)\n",
    "    else:\n",
    "        if (len(y_idx1)!=0 and len(x_idx1)!=0):\n",
    "            for y,x in zip(y_idx1, x_idx1):\n",
    "                pv_anomaly1 = ((pv_field[y,x]-clim18[y,x])/std18[y,x])\n",
    "        else:\n",
    "            pv_anomaly1 = np.repeat(float('NaN'),5)\n",
    "             \n",
    "    if dates[q].hour == 0:\n",
    "        if (len(y_idx2)!=0 and len(x_idx2)!=0):\n",
    "            for y,x in zip(y_idx2, x_idx2):\n",
    "                pv_anomaly2 = ((pv_field[y,x]-clim0[y,x])/std0[y,x])\n",
    "        else:\n",
    "            pv_anomaly2 = np.repeat(float('NaN'),5)\n",
    "    elif dates[q].hour == 6:\n",
    "        if (len(y_idx2)!=0 and len(x_idx2)!=0):\n",
    "            for y,x in zip(y_idx2, x_idx2):\n",
    "                pv_anomaly2 = ((pv_field[y,x]-clim6[y,x])/std6[y,x])\n",
    "        else:\n",
    "            pv_anomaly2 = np.repeat(float('NaN'),5)\n",
    "    elif dates[q].hour == 12:\n",
    "        if (len(y_idx2)!=0 and len(x_idx2)!=0):\n",
    "            for y,x in zip(y_idx2, x_idx2):\n",
    "                pv_anomaly2 = ((pv_field[y,x]-clim12[y,x])/std12[y,x])\n",
    "        else:\n",
    "            pv_anomaly2 = np.repeat(float('NaN'),5)\n",
    "    else:\n",
    "        if (len(y_idx2)!=0 and len(x_idx2)!=0):\n",
    "            for y,x in zip(y_idx2, x_idx2):\n",
    "                pv_anomaly2 = ((pv_field[y,x]-clim18[y,x])/std18[y,x])\n",
    "        else:\n",
    "            pv_anomaly2 = np.repeat(float('NaN'),5)\n",
    "    # ......................................................................\n",
    "    \n",
    "    # Test for NaNs. I remove NaNs here, why????\n",
    "    if pv_anomaly1.size>1:\n",
    "        if np.isnan(pv_anomaly1.any())== True:\n",
    "            pv_anomaly1[np.isnan(pv_anomaly1)]=0\n",
    "        elif np.isnan(pv_anomaly1.all())== True:\n",
    "            pv_anomaly1 = 0\n",
    "        else:\n",
    "            pv_anomaly1 = pv_anomaly1\n",
    "    else:\n",
    "        if np.isnan(pv_anomaly1.any())== True:\n",
    "            pv_anomaly1[np.isnan(pv_anomaly1)]=0\n",
    "        elif np.isnan(pv_anomaly1)== True:\n",
    "            pv_anomaly1 = 0\n",
    "        else:\n",
    "            pv_anomaly1 = pv_anomaly1 \n",
    "    pv_anom1 = np.sum(np.float64(pv_anomaly1))\n",
    "    \n",
    "    if pv_anomaly2.size>1:\n",
    "        if np.isnan(pv_anomaly2.any())== True:\n",
    "            pv_anomaly2[np.isnan(pv_anomaly2)]=0\n",
    "        elif np.isnan(pv_anomaly2.all())== True:\n",
    "            pv_anomaly2 = 0\n",
    "        else:\n",
    "            pv_anomaly2 = pv_anomaly2\n",
    "    else:\n",
    "        if np.isnan(pv_anomaly2.any())== True:\n",
    "            pv_anomaly2[np.isnan(pv_anomaly2)]=0\n",
    "        elif np.isnan(pv_anomaly2)== True:\n",
    "            pv_anomaly2 = 0\n",
    "        else:\n",
    "            pv_anomaly2 = pv_anomaly2\n",
    "    pv_anom2 = np.sum(np.float64(pv_anomaly2))        \n",
    "        \n",
    "    # Sum pv anomaly over streamers to obtain 6hrly activity    \n",
    "    daily_total = np.nansum([pv_anom1, pv_anom2])\n",
    "    \n",
    "    # Append to daily index\n",
    "    pvsi_full.append(daily_total)\n",
    "    print('Field at ', q, 'is complete.')\n",
    "    \n",
    "subdaily_pvsi = np.array(pvsi_full)\n",
    "np.savetxt('/home/jjpjones/pvs_vws_indices/pvsi_full_6hrly_updates.txt',subdaily_pvsi,delimiter=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

